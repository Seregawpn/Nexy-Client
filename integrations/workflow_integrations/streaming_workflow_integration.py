#!/usr/bin/env python3
"""
StreamingWorkflowIntegration - —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–º: —Ç–µ–∫—Å—Ç ‚Üí –∞—É–¥–∏–æ ‚Üí –∫–ª–∏–µ–Ω—Ç
"""

import logging
import os
from typing import Dict, Any, AsyncGenerator, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class StreamingWorkflowIntegration:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ—Ç–æ–∫–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏: –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ ‚Üí –æ–±—Ä–∞–±–æ—Ç–∫–∞ ‚Üí –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ ‚Üí —Å—Ç—Ä–∏–º–∏–Ω–≥ –∫–ª–∏–µ–Ω—Ç—É
    """
    
    def __init__(self, text_processor=None, audio_processor=None, memory_workflow=None, text_filter_manager=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StreamingWorkflowIntegration
        
        Args:
            text_processor: –ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
            audio_processor: –ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ
            memory_workflow: Workflow –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞–º—è—Ç—å—é
            text_filter_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
        """
        self.text_processor = text_processor
        self.audio_processor = audio_processor
        self.memory_workflow = memory_workflow
        self.text_filter_manager = text_filter_manager
        self.is_initialized = False
        
        # –ï–¥–∏–Ω–∞—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∞—è –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è –∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ñ–ª–∞—à–∏–Ω–≥–∞ (–¥–ª—è —Ç–µ–∫—Å—Ç–∞ –∏ TTS –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
        self._stream_buffer: str = ""
        self._has_emitted: bool = False
        self._pending_segment: str = ""
        self._processed_sentences: set = set()  # –î–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        # –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ (STREAM_*), —Å –±—ç–∫–≤–∞—Ä–¥-—Ñ–æ–ª–ª–±–µ–∫–æ–º –Ω–∞ TTS_* (—É–º–µ–Ω—å—à–µ–Ω—ã –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è)
        self.stream_min_chars: int = int(os.getenv("STREAM_MIN_CHARS", os.getenv("TTS_MIN_CHARS", "15")))
        self.stream_min_words: int = int(os.getenv("STREAM_MIN_WORDS", os.getenv("TTS_MIN_WORDS", "3")))
        self.stream_first_sentence_min_words: int = int(os.getenv("STREAM_FIRST_SENTENCE_MIN_WORDS", os.getenv("TTS_FIRST_SENTENCE_MIN_WORDS", "2")))
        self.stream_punct_flush_strict: bool = os.getenv("STREAM_PUNCT_FLUSH_STRICT", os.getenv("TTS_PUNCT_FLUSH_STRICT", "true")).lower() == "true"
        self.sentence_joiner: str = " "
        self.end_punctuations = ('.', '!', '?')
        
        logger.info("StreamingWorkflowIntegration —Å–æ–∑–¥–∞–Ω")
    
    async def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è StreamingWorkflowIntegration...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥—É–ª–µ–π
            if not self.text_processor:
                logger.warning("‚ö†Ô∏è TextProcessor –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.audio_processor:
                logger.warning("‚ö†Ô∏è AudioProcessor –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.memory_workflow:
                logger.warning("‚ö†Ô∏è MemoryWorkflow –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            if not self.text_filter_manager:
                logger.warning("‚ö†Ô∏è TextFilterManager –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            
            self.is_initialized = True
            logger.info("‚úÖ StreamingWorkflowIntegration –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ StreamingWorkflowIntegration: {e}")
            return False
    
    async def process_request_streaming(self, request_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∞—É–¥–∏–æ —Å—Ç—Ä–∏–º—è—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""
        if not self.is_initialized:
            logger.error("‚ùå StreamingWorkflowIntegration –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            yield {
                'success': False,
                'error': 'StreamingWorkflowIntegration not initialized',
                'text_response': '',
            }
            return

        session_id = request_data.get('session_id', 'unknown')
        try:
            logger.info(f"üîÑ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {session_id}")
            logger.info(f"‚Üí Input text len={len(request_data.get('text','') or '')}, has_screenshot={bool(request_data.get('screenshot'))}")
            logger.info(f"‚Üí Input text content: '{request_data.get('text', '')[:100]}...'")

            logger.info("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ú–û–î–£–õ–ï–ô:")
            logger.info(f"   ‚Üí text_processor: {self.text_processor is not None}")
            logger.info(f"   ‚Üí audio_processor: {self.audio_processor is not None}")
            if self.text_processor:
                logger.info(f"   ‚Üí text_processor.is_initialized: {getattr(self.text_processor, 'is_initialized', 'NO_ATTR')}")
            if self.audio_processor:
                logger.info(f"   ‚Üí audio_processor.is_initialized: {getattr(self.audio_processor, 'is_initialized', 'NO_ATTR')}")

            hardware_id = request_data.get('hardware_id', 'unknown')
            memory_context = await self._get_memory_context_parallel(hardware_id)

            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–µ–¥ –Ω–æ–≤–æ–π —Å–µ—Å—Å–∏–µ–π,
            # –∏–Ω–∞—á–µ –æ—Å—Ç–∞—Ç–∫–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–∑—ã–≤–∞—é—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
            self._stream_buffer = ""
            self._pending_segment = ""
            self._has_emitted = False
            self._processed_sentences.clear()

            captured_segments: list[str] = []
            input_sentence_counter = 0
            emitted_segment_counter = 0
            total_audio_chunks = 0
            total_audio_bytes = 0
            sentence_audio_map: dict[int, int] = {}

            async for sentence in self._iter_processed_sentences(
                request_data.get('text', ''),
                request_data.get('screenshot'),
                memory_context
            ):
                input_sentence_counter += 1
                logger.info(f"üìù In sentence #{input_sentence_counter}: '{sentence[:120]}{'...' if len(sentence) > 120 else ''}' (len={len(sentence)})")

                # –ï–¥–∏–Ω–∞—è –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è: –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º, –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                sanitized = await self._sanitize_for_tts(sentence)
                if sanitized:
                    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–±–æ–ª–µ–µ –º—è–≥–∫–∞—è)
                    sanitized_hash = hash(sanitized.strip())
                    if sanitized_hash in self._processed_sentences:
                        logger.debug(f"üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: '{sanitized[:50]}...'")
                        continue
                    self._processed_sentences.add(sanitized_hash)
                    
                    self._stream_buffer = (f"{self._stream_buffer}{self.sentence_joiner}{sanitized}" if self._stream_buffer else sanitized)

                complete_sentences, remainder = await self._split_complete_sentences(self._stream_buffer)
                self._stream_buffer = remainder

                for complete in complete_sentences:
                    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ –ø–æ—Ä–æ–≥–æ–≤
                    candidate = complete if not self._pending_segment else f"{self._pending_segment}{self.sentence_joiner}{complete}"
                    words_count = await self._count_meaningful_words(candidate)
                    if (not self._has_emitted and (words_count >= self.stream_first_sentence_min_words or len(candidate) >= self.stream_min_chars)) or \
                       (self._has_emitted and (words_count >= self.stream_min_words or len(candidate) >= self.stream_min_chars)):
                        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π)
                        to_emit = candidate.strip()
                        if len(to_emit) > 10:  # –¢–æ–ª—å–∫–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
                            complete_hash = hash(to_emit)
                            if complete_hash in self._processed_sentences:
                                logger.debug(f"üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç: '{to_emit[:50]}...'")
                                continue
                            self._processed_sentences.add(complete_hash)
                        
                        # –ì–æ—Ç–æ–≤ –∫ —ç–º–∏—Å—Å–∏–∏
                        emitted_segment_counter += 1
                        self._pending_segment = ""
                        self._has_emitted = True

                        # –¢–µ–∫—Å—Ç
                        captured_segments.append(to_emit)
                        yield {
                            'success': True,
                            'text_response': to_emit,
                            'sentence_index': emitted_segment_counter
                        }

                        # –ê—É–¥–∏–æ (–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–∞—é—â—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –¥–ª—è TTS)
                        tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                        sentence_audio_chunks = 0
                        async for audio_chunk in self._stream_audio_for_sentence(tts_text, emitted_segment_counter):
                            if not audio_chunk:
                                continue
                            sentence_audio_chunks += 1
                            total_audio_chunks += 1
                            total_audio_bytes += len(audio_chunk)
                            yield {
                                'success': True,
                                'audio_chunk': audio_chunk,
                                'sentence_index': emitted_segment_counter,
                                'audio_chunk_index': sentence_audio_chunks
                            }

                        sentence_audio_map[emitted_segment_counter] = sentence_audio_chunks
                        logger.info(
                            f"üéß Segment #{emitted_segment_counter} ‚Üí audio_chunks={sentence_audio_chunks}, total_audio_chunks={total_audio_chunks}, total_bytes={total_audio_bytes}"
                        )
                    else:
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫–æ–ø–∏—Ç—å
                        self._pending_segment = candidate

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–ª–∞—à: —Å–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –±—É—Ñ–µ—Ä–∞
            if self._stream_buffer:
                complete_sentences, remainder = await self._split_complete_sentences(self._stream_buffer)
                self._stream_buffer = remainder
                for complete in complete_sentences:
                    candidate = complete if not self._pending_segment else f"{self._pending_segment}{self.sentence_joiner}{complete}"
                    words_count = await self._count_meaningful_words(candidate)
                    if (not self._has_emitted and (words_count >= self.stream_first_sentence_min_words or len(candidate) >= self.stream_min_chars)) or \
                       (self._has_emitted and (words_count >= self.stream_min_words or len(candidate) >= self.stream_min_chars)):
                        emitted_segment_counter += 1
                        to_emit = candidate.strip()
                        self._pending_segment = ""
                        self._has_emitted = True
                        captured_segments.append(to_emit)
                        yield {'success': True, 'text_response': to_emit, 'sentence_index': emitted_segment_counter}
                        tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                        sentence_audio_chunks = 0
                        async for audio_chunk in self._stream_audio_for_sentence(tts_text, emitted_segment_counter):
                            if not audio_chunk:
                                continue
                            sentence_audio_chunks += 1
                            total_audio_chunks += 1
                            total_audio_bytes += len(audio_chunk)
                            yield {'success': True, 'audio_chunk': audio_chunk, 'sentence_index': emitted_segment_counter, 'audio_chunk_index': sentence_audio_chunks}
                        sentence_audio_map[emitted_segment_counter] = sentence_audio_chunks
                        logger.info(f"üéß Final segment #{emitted_segment_counter} ‚Üí audio_chunks={sentence_audio_chunks}, total_audio_chunks={total_audio_chunks}, total_bytes={total_audio_bytes}")
                    else:
                        self._pending_segment = candidate

            # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –∞–≥—Ä–µ–≥–∞—Ç, –º–æ–∂–Ω–æ —Ñ–æ—Ä—Å-—Ñ–ª–∞—à, –µ—Å–ª–∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π
            force_max = int(os.getenv("STREAM_FORCE_FLUSH_MAX_CHARS", "0") or 0)
            if self._pending_segment and force_max > 0 and len(self._pending_segment) >= force_max:
                emitted_segment_counter += 1
                to_emit = self._pending_segment
                self._pending_segment = ""
                self._has_emitted = True
                captured_segments.append(to_emit)
                yield {'success': True, 'text_response': to_emit, 'sentence_index': emitted_segment_counter}
                tts_text = to_emit if to_emit.endswith(self.end_punctuations) else f"{to_emit}."
                sentence_audio_chunks = 0
                async for audio_chunk in self._stream_audio_for_sentence(tts_text, emitted_segment_counter):
                    if not audio_chunk:
                        continue
                    sentence_audio_chunks += 1
                    total_audio_chunks += 1
                    total_audio_bytes += len(audio_chunk)
                    yield {'success': True, 'audio_chunk': audio_chunk, 'sentence_index': emitted_segment_counter, 'audio_chunk_index': sentence_audio_chunks}
                sentence_audio_map[emitted_segment_counter] = sentence_audio_chunks
                logger.info(f"üéß Forced final segment #{emitted_segment_counter} ‚Üí audio_chunks={sentence_audio_chunks}, total_audio_chunks={total_audio_chunks}, total_bytes={total_audio_bytes}")

            full_text = " ".join(captured_segments).strip()

            logger.info(
                f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ: segments={emitted_segment_counter}, audio_chunks={total_audio_chunks}, total_bytes={total_audio_bytes}"
            )
            yield {
                'success': True,
                'text_full_response': full_text,
                'sentences_processed': emitted_segment_counter,
                'audio_chunks_processed': total_audio_chunks,
                'audio_bytes_processed': total_audio_bytes,
                'sentence_audio_map': sentence_audio_map,
                'is_final': True
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ {session_id}: {e}")
            yield {
                'success': False,
                'error': str(e),
                'text_response': '',
            }

    async def _get_memory_context_parallel(self, hardware_id: str) -> Optional[Dict[str, Any]]:
        """
        –ù–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏
        
        Args:
            hardware_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        """
        try:
            if not self.memory_workflow:
                logger.debug("MemoryWorkflow –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏")
                return None
            
            logger.debug(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è {hardware_id}")
            memory_context = await self.memory_workflow.get_memory_context_parallel(hardware_id)
            
            if memory_context:
                logger.debug(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏: {len(memory_context)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            else:
                logger.debug("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ –ø—É—Å—Ç")
            
            return memory_context
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç–∏: {e}")
            return None

    async def _iter_processed_sentences(
        self,
        text: str,
        screenshot: Optional[str],
        memory_context: Optional[Dict[str, Any]]
    ) -> AsyncGenerator[str, None]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á—ë—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞."""
        enriched_text = self._enrich_with_memory(text, memory_context)

        screenshot_data: Optional[bytes] = None
        if screenshot:
            import base64
            try:
                screenshot_data = base64.b64decode(screenshot)
                logger.info(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω: {len(screenshot_data)} bytes")
            except Exception as decode_error:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç: {decode_error}")
                screenshot_data = None

        yielded_any = False
        if self.text_processor and hasattr(self.text_processor, 'process_text_streaming'):
            logger.info(f"üîÑ –°—Ç—Ä–∏–º–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ TextProcessor: '{enriched_text[:80]}...'")
            try:
                async for processed_sentence in self.text_processor.process_text_streaming(enriched_text, screenshot_data):
                    sentence = (processed_sentence or '').strip()
                    if sentence:
                        yielded_any = True
                        logger.debug(f"üì® TextProcessor sentence: '{sentence[:120]}...'")
                        yield sentence
            except Exception as processing_error:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ TextProcessor: {processing_error}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback")

        if not yielded_any:
            logger.debug("‚ö†Ô∏è TextProcessor –Ω–µ –≤–µ—Ä–Ω—É–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ä–∞–∑–±–∏–≤–∫—É")
            for fallback_sentence in self._split_into_sentences(enriched_text):
                if fallback_sentence:
                    yield fallback_sentence

    async def _sanitize_for_tts(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            return ""

        if self.text_filter_manager:
            try:
                result = await self.text_filter_manager.clean_text(text, {
                    "remove_special_chars": True,
                    "remove_extra_whitespace": True,
                    "normalize_unicode": True,
                    "remove_control_chars": True
                })
                if result.get("success") and result.get("cleaned_text") is not None:
                    return result.get("cleaned_text", "").strip()
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ TextFilterManager: %s", err)

        return text.strip()

    async def _split_complete_sentences(self, text: str) -> tuple[list[str], str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            return [], ""

        if self.text_filter_manager:
            try:
                result = await self.text_filter_manager.split_sentences(text)
                if result.get("success"):
                    return result.get("sentences", []), result.get("remainder", "")
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ TextFilterManager: %s", err)

        stripped = text.strip()
        return ([stripped] if stripped else [], "")

    async def _count_meaningful_words(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á—ë—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        """
        if not text:
            return 0

        if self.text_filter_manager:
            try:
                return self.text_filter_manager.count_meaningful_words(text)
            except Exception as err:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á—ë—Ç–∞ —Å–ª–æ–≤ —á–µ—Ä–µ–∑ TextFilterManager: %s", err)

        return len([w for w in text.split() if w.strip()])

    def _enrich_with_memory(self, text: str, memory_context: Optional[Dict[str, Any]]) -> str:
        """
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–∞–º—è—Ç–∏
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            memory_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏
        """
        if not memory_context:
            return text
        
        try:
            memory_info = memory_context.get('recent_context', '') if memory_context else ''
            if memory_info:
                enriched_text = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {memory_info}\n\n{text}"
                logger.debug("–¢–µ–∫—Å—Ç –æ–±–æ–≥–∞—â–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–∞–º—è—Ç–∏")
                return enriched_text
            return text
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–∞–º—è—Ç—å—é: {e}")
            return text

    async def _stream_audio_for_sentence(self, sentence: str, sentence_index: int) -> AsyncGenerator[bytes, None]:
        """–°—Ç—Ä–∏–º–∏—Ç –∞—É–¥–∏–æ —á–∞–Ω–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        if not sentence.strip():
            return
        if not self.audio_processor:
            logger.warning("‚ö†Ô∏è AudioProcessor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∞—É–¥–∏–æ")
            return
        if not hasattr(self.audio_processor, 'generate_speech_streaming'):
            logger.warning("‚ö†Ô∏è AudioProcessor –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç generate_speech_streaming")
            return
        if hasattr(self.audio_processor, 'is_initialized') and not self.audio_processor.is_initialized:
            logger.warning("‚ö†Ô∏è AudioProcessor –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return

        try:
            logger.info(f"üîä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: '{sentence[:80]}...'")
            chunk_count = 0
            async for audio_chunk in self.audio_processor.generate_speech_streaming(sentence):
                if audio_chunk:
                    chunk_count += 1
                    logger.info(f"üîä Audio chunk #{chunk_count} –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {len(audio_chunk)} bytes")
                    yield audio_chunk
            logger.info(f"‚úÖ –ê—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {chunk_count} —á–∞–Ω–∫–æ–≤")
        except Exception as audio_error:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è #{sentence_index}: {audio_error}")
    
    def _split_into_sentences(self, text: str) -> list[str]:
        """
        –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
            import re
            sentences = re.split(r'[.!?]+', text)
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            clean_sentences = [s.strip() for s in sentences if s.strip()]
            
            logger.debug(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(clean_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
            return clean_sentences
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return [text]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            logger.info("–û—á–∏—Å—Ç–∫–∞ StreamingWorkflowIntegration...")
            self.is_initialized = False
            logger.info("‚úÖ StreamingWorkflowIntegration –æ—á–∏—â–µ–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ StreamingWorkflowIntegration: {e}")
