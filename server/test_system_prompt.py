#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥–∞—á–∏ System Prompt –≤ LangChain
"""

import asyncio
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

async def test_system_prompt():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–∞—á—É System Prompt –≤ LangChain"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
    if not os.environ.get("GEMINI_API_KEY"):
        print("‚ùå GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        return
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash-lite",
            google_api_key=os.environ.get("GEMINI_API_KEY"),
            temperature=0.7,
            max_output_tokens=1024,
            streaming=False  # –û—Ç–∫–ª—é—á–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –¥–ª—è —Ç–µ—Å—Ç–∞
        )
        
        print("‚úÖ LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        
        # –¢–µ—Å—Ç 1: –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
        print("\nüß™ –¢–µ—Å—Ç 1: –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç")
        messages_text = [
            SystemMessage(content="You are a helpful assistant. Always respond in English and be very brief."),
            HumanMessage(content="–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?")
        ]
        
        response_text = await llm.ainvoke(messages_text)
        print(f"üìù –û—Ç–≤–µ—Ç: {response_text.content}")
        
        # –¢–µ—Å—Ç 2: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å (–±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã)
        print("\nüß™ –¢–µ—Å—Ç 2: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å")
        messages_multimodal = [
            SystemMessage(content="You are a helpful assistant. Always respond in English and be very brief."),
            HumanMessage(content=[
                {"type": "text", "text": "Describe what you see in this image"},
                {"type": "image_url", "image_url": {"url": "https://example.com/test.jpg"}}
            ])
        ]
        
        response_multimodal = await llm.ainvoke(messages_multimodal)
        print(f"üñºÔ∏è –û—Ç–≤–µ—Ç: {response_multimodal.content}")
        
        print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_system_prompt())
