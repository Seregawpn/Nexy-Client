# Техническая структура продукта: "Голосовой со-пилот для незрячих"

## Архитектура системы

### Общий принцип: модульная система "мозаика"
Каждый модуль имеет чёткий интерфейс и может быть заменён на любой совместимый инструмент без изменения остальной системы.

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   UI Interface  │    │  State Manager  │    │  Main Controller│
│   (меню-бар)    │◄──►│   (настройки)   │◄──►│  (оркестратор)  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │                       │
                                ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  Speech Input   │    │   TTS Engine    │    │   LLM Client    │
│   (ASR)         │◄──►│   (синтез)      │◄──►│   (анализ)      │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │                       │
                                ▼                       ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│ UI Capture      │    │ Image Processor │    │  Speech Proc    │
│ (захват экрана) │◄──►│ (обработка)     │◄──►│ (шум/эхо)      │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

## Модули системы

### 1. UI Capture Module (захват экрана)

**Назначение**: захват активного окна macOS и получение метаданных.

**Требования**:
- Вход: активное окно
- Выход: изображение (≤2MB) + метаданные
- Форматы: WebP/JPEG/PNG
- Разрешение: ≤2048px по ширине
- Метаданные: `{appName, windowTitle, bounds, scale, timestamp}`

**Интерфейс**:
```python
class UICaptureInterface:
    def capture_active_window(self) -> Tuple[bytes, dict]:
        """
        Захватывает активное окно
        Returns: (image_bytes, metadata)
        """
        pass
    
    def get_window_info(self) -> dict:
        """Возвращает информацию об активном окне"""
        pass
    
    def is_capture_available(self) -> bool:
        """Проверяет доступность захвата"""
        pass
```

**Возможные реализации**:
- PyObjC + Core Graphics (рекомендуемый)
- Screenshot API
- Accessibility API

**Метрики производительности**:
- Время захвата: ≤300 мс
- Размер изображения: ≤2MB
- Частота кадров: 1 FPS (по запросу)

### 2. Image Processor Module (обработка изображений)

**Назначение**: оптимизация изображений для LLM и хранения.

**Требования**:
- Вход: изображение (≤2MB)
- Выход: оптимизированное изображение (≤1MB)
- Форматы: WebP (приоритет), JPEG, PNG
- Максимальная ширина: 1024px
- Качество: настраиваемое (0.6-0.9)

**Интерфейс**:
```python
class ImageProcessorInterface:
    def process_image(self, image_bytes: bytes, 
                     target_format: str = "webp",
                     max_width: int = 1024, 
                     quality: float = 0.8) -> bytes:
        """Обрабатывает изображение согласно требованиям"""
        pass
    
    def get_supported_formats(self) -> List[str]:
        """Возвращает поддерживаемые форматы"""
        pass
    
    def estimate_size(self, image_bytes: bytes, 
                     target_format: str, 
                     max_width: int, 
                     quality: float) -> int:
        """Оценивает размер выходного файла"""
        pass
```

**Возможные реализации**:
- Pillow (PIL) - рекомендуемый
- OpenCV
- ImageMagick
- Системные библиотеки macOS

**Метрики производительности**:
- Время обработки: ≤200 мс
- Коэффициент сжатия: ≥2:1
- Поддержка форматов: ≥3

### 3. LLM Client Module (анализ и генерация)

**Назначение**: анализ экрана и генерация структурированных ответов.

**Требования**:
- Вход: изображение (≤1MB) + метаданные + промпт
- Выход: структурированный JSON ответ
- Функции: анализ экрана, классификация, генерация описаний
- Формат ответа: строго определённая схема

**Интерфейс**:
```python
class LLMClientInterface:
    def analyze_screen(self, image_bytes: bytes, 
                      metadata: dict, 
                      prompt: str) -> dict:
        """Анализирует экран и возвращает структурированный ответ"""
        pass
    
    def get_response_format(self) -> dict:
        """Возвращает ожидаемый формат ответа"""
        pass
    
    def is_available(self) -> bool:
        """Проверяет доступность LLM"""
        pass
    
    def get_usage_stats(self) -> dict:
        """Возвращает статистику использования"""
        pass
```

**Схема ответа**:
```json
{
  "summary": "краткое описание экрана (1-2 фразы)",
  "actions": [
    "действие 1 (глаголом)",
    "действие 2 (глаголом)",
    "действие 3 (глаголом)"
  ],
  "confidence": 0.85,
  "screen_type": "search_results|listing|form|article|dialog",
  "clarifications": [],
  "metadata": {
    "processing_time_ms": 1200,
    "model_used": "gpt-4-vision-preview"
  }
}
```

**Возможные реализации**:
- OpenAI GPT-4V (рекомендуемый)
- Google Gemini Pro Vision
- Anthropic Claude 3.5 Sonnet
- Локальные модели (Llama, Mistral)

**Метрики производительности**:
- Время анализа: ≤1500 мс
- Точность описания: ≥90%
- Размер ответа: ≤2KB

### 4. Speech Recognition Module (распознавание речи)

**Назначение**: локальное распознавание речи пользователя.

**Требования**:
- Вход: аудио поток (16kHz, mono, float32)
- Выход: распознанный текст + уверенность
- Функции: распознавание команд, обработка вопросов
- Качество: высокая точность для команд

**Интерфейс**:
```python
class SpeechRecognitionInterface:
    def listen_for_command(self, timeout: float = 10.0) -> str:
        """Слушает команду пользователя, возвращает распознанный текст"""
        pass
    
    def is_listening(self) -> bool:
        """Проверяет, слушает ли система"""
        pass
    
    def stop_listening(self):
        """Останавливает прослушивание"""
        pass
    
    def get_supported_languages(self) -> List[str]:
        """Возвращает поддерживаемые языки"""
        pass
    
    def set_language(self, language: str):
        """Устанавливает язык распознавания"""
        pass
```

**Возможные реализации**:
- SpeechRecognition + Google Speech API - рекомендуемый
- Whisper (локальный)
- Microsoft Speech SDK

**Метрики производительности**:
- Время распознавания: ≤1000 мс
- Точность команд: ≥95%
- Поддержка языков: RU, EN
- Работа без интернета: частично

### 5. TTS Engine Module (синтез речи)

**Назначение**: преобразование текста в речь с настройками.

**Требования**:
- Вход: текст (RU/EN) + настройки
- Выход: аудио поток (16kHz, mono)
- Функции: streaming, управление голосами, настройки
- Поддержка языков: RU, EN

**Интерфейс**:
```python
class TTSEngineInterface:
    def synthesize(self, text: str, language: str = "ru", 
                  speed: float = 1.0, voice: str = "default") -> bytes:
        """Синтезирует речь из текста"""
        pass
    
    def stream_synthesize(self, text: str, language: str = "ru", 
                         speed: float = 1.0, voice: str = "default") -> Generator[bytes, None, None]:
        """Синтезирует речь потоком"""
        pass
    
    def get_available_voices(self, language: str) -> List[str]:
        """Возвращает доступные голоса для языка"""
        pass
    
    def set_voice(self, voice_name: str):
        """Устанавливает голос"""
        pass
    
    def get_speech_settings(self) -> dict:
        """Возвращает текущие настройки речи"""
        pass
```

**Возможные реализации**:
- Системный TTS (NSSpeechSynthesizer) - рекомендуемый
- Google TTS
- Microsoft Speech TTS
- Локальные модели (Coqui TTS)

**Метрики производительности**:
- Время до начала речи: ≤300 мс
- Качество речи: естественное
- Поддержка языков: ≥2
- Количество голосов: ≥4

### 6. State Manager Module (управление состоянием)

**Назначение**: хранение настроек, кэш, история команд.

**Требования**:
- Хранение: настройки, кэш изображений, история
- Функции: CRUD операции, экспорт/импорт, очистка
- Безопасность: шифрование чувствительных данных
- Производительность: быстрый доступ

**Интерфейс**:
```python
class StateManagerInterface:
    def save_setting(self, key: str, value: any):
        """Сохраняет настройку"""
        pass
    
    def get_setting(self, key: str, default: any = None) -> any:
        """Получает настройку"""
        pass
    
    def cache_image(self, image_bytes: bytes, metadata: dict) -> str:
        """Кэширует изображение, возвращает ID"""
        pass
    
    def get_cached_image(self, image_id: str) -> Tuple[bytes, dict]:
        """Получает кэшированное изображение"""
        pass
    
    def cleanup_old_cache(self, max_age_hours: int = 24):
        """Очищает старый кэш"""
        pass
    
    def export_settings(self) -> bytes:
        """Экспортирует настройки"""
        pass
    
    def import_settings(self, settings_data: bytes) -> bool:
        """Импортирует настройки"""
        pass
```

**Возможные реализации**:
- SQLite + файловая система (рекомендуемый)
- JSON файлы
- Redis (для продакшена)

**Метрики производительности**:
- Время чтения настроек: ≤10 мс
- Время записи настроек: ≤50 мс
- Размер кэша: ≤100MB
- Время очистки: ≤100 мс

### 7. UI Interface Module (пользовательский интерфейс)

**Назначение**: меню-бар, горячие клавиши, настройки.

**Требования**:
- Меню-бар приложение
- Горячие клавиши (настраиваемые)
- Настройки пользователя
- Статус и уведомления
- Доступность для незрячих

**Интерфейс**:
```python
class UIInterface:
    def show_menu(self):
        """Показывает меню приложения"""
        pass
    
    def register_hotkey(self, key_combination: str, callback: callable):
        """Регистрирует горячую клавишу"""
        pass
    
    def unregister_hotkey(self, key_combination: str):
        """Отменяет регистрацию горячей клавиши"""
        pass
    
    def show_notification(self, title: str, message: str):
        """Показывает уведомление"""
        pass
    
    def open_settings(self):
        """Открывает окно настроек"""
        pass
    
    def update_status(self, status: str):
        """Обновляет статус приложения"""
        pass
```

**Возможные реализации**:
- Rumps (PyObjC) - рекомендуемый
- PyQt/PySide
- Tkinter
- Системные API macOS

**Метрики производительности**:
- Время отклика UI: ≤100 мс
- Поддержка горячих клавиш: ≥10
- Размер меню: ≤20 пунктов

## Основной оркестратор

### Класс, который собирает все модули:

```python
class AssistantOrchestrator:
    def __init__(self, config: dict):
        # Инициализируем модули через интерфейсы
        self.ui_capture = self._init_module('ui_capture', config)
        self.image_processor = self._init_module('image_processor', config)
        self.llm_client = self._init_module('llm_client', config)
        self.speech_recognition = self._init_module('speech_recognition', config)
        self.tts_engine = self._init_module('tts_engine', config)
        self.state_manager = self._init_module('state_manager', config)
        self.space_controller = self._init_module('space_controller', config)
        
        # Состояние системы
        self.state = 'inactive'  # inactive, active, interrupted, listening, processing
        self.is_speaking = False
        self.last_screenshot = None
        self.last_response = None
        
    def process_screen_request(self):
        """Основной поток: 'Что на экране?'"""
        try:
            self.state = 'processing'
            self.is_speaking = True
            
            # 1. Захват экрана
            image_bytes, metadata = self.ui_capture.capture_active_window()
            
            # 2. Обработка изображения
            processed_image = self.image_processor.process_image(
                image_bytes, 
                target_format="webp", 
                max_width=1024, 
                quality=0.8
            )
            
            # 3. Анализ LLM
            response = self.llm_client.analyze_screen(
                processed_image, 
                metadata, 
                self._get_screen_prompt()
            )
            
            # 4. Сохранение в кэш
            image_id = self.state_manager.cache_image(processed_image, metadata)
            
            # 5. Синтез речи
            audio = self.tts_engine.synthesize(
                self._format_response_for_speech(response)
            )
            
            # 6. Воспроизведение
            self._play_audio(audio)
            
            # 7. Обновление состояния
            self.last_screenshot = image_id
            self.last_response = response
            self.state = 'active'
            self.is_speaking = False
            
        except Exception as e:
            self._handle_error(e)
            self.state = 'inactive'
            self.is_speaking = False
    
    def process_what_changed_request(self):
        """Обработка: 'Что изменилось?'"""
        if not self.last_screenshot:
            return "Нет предыдущего снимка для сравнения"
            
        # Получаем текущий снимок
        current_image, current_metadata = self.ui_capture.capture_active_window()
        current_processed = self.image_processor.process_image(current_image)
        
        # Получаем предыдущий снимок
        previous_image, previous_metadata = self.state_manager.get_cached_image(
            self.last_screenshot
        )
        
        # Анализируем изменения через LLM
        diff_response = self.llm_client.analyze_changes(
            previous_image, 
            current_processed, 
            previous_metadata, 
            current_metadata
        )
        
        # Озвучиваем изменения
        audio = self.tts_engine.synthesize(diff_response['summary'])
        self._play_audio(audio)
        
        return diff_response
        
    def handle_space_press(self, press_duration: float):
        """Обрабатывает нажатие пробела"""
        
        if press_duration >= 0.6:  # Долгое нажатие
            if self.state == 'inactive':
                # Активируем ассистента
                self.activate_assistant()
            elif self.state == 'active':
                # Выключаем ассистента
                self.deactivate_assistant()
        else:  # Короткое нажатие
            if self.state == 'active' and self.is_speaking:
                # Перебиваем речь
                self.interrupt_speech()
            elif self.state == 'interrupted':
                # Слушаем команду
                self.listen_for_command()
                
    def activate_assistant(self):
        """Активирует ассистента"""
        self.state = 'active'
        self.process_screen_request()
        
    def deactivate_assistant(self):
        """Выключает ассистента"""
        if self.is_speaking:
            self.tts_engine.stop()
        self.state = 'inactive'
        self.is_speaking = False
        
    def interrupt_speech(self):
        """Перебивает речь ассистента"""
        self.tts_engine.stop()
        self.is_speaking = False
        self.state = 'interrupted'
        
    def listen_for_command(self):
        """Слушает команду пользователя"""
        self.state = 'listening'
        
        # Распознаём речь
        command = self.speech_recognition.listen_for_command()
        
        if command and command not in ["Таймаут ожидания речи", "Речь не распознана"]:
            # Обрабатываем команду
            self.process_command(command)
        else:
            # Ошибка распознавания
            self.tts_engine.synthesize("Не удалось распознать речь. Попробуйте ещё раз.")
            self.state = 'active'
            
    def process_command(self, command: str):
        """Обрабатывает голосовую команду"""
        command_lower = command.lower()
        
        if "что на экране" in command_lower:
            self.process_screen_request()
        elif "повтори" in command_lower:
            self.repeat_last_response()
        elif "стоп" in command_lower:
            self.deactivate_assistant()
        else:
            # Неизвестная команда
            self.tts_engine.synthesize(f"Команда '{command}' не распознана. Попробуйте: 'что на экране', 'повтори', 'стоп'")
            self.state = 'active'
            
    def repeat_last_response(self):
        """Повторяет последний ответ"""
        if self.last_response:
            audio = self.tts_engine.synthesize(
                self._format_response_for_speech(self.last_response)
            )
            self._play_audio(audio)
        else:
            self.tts_engine.synthesize("Нет предыдущего ответа для повторения")
        self.state = 'active'
    
    def _init_module(self, module_name: str, config: dict):
        """Инициализирует модуль по конфигурации"""
        module_class = config['modules'][module_name]
        module_config = config.get(module_name, {})
        return module_class(module_config)
    
    def _get_screen_prompt(self) -> str:
        """Возвращает промпт для анализа экрана"""
        return """
        Ты ассистент для незрячих. Проанализируй скриншот и дай:
        1. Краткое описание экрана (1-2 фразы)
        2. 3-5 конкретных действий (глаголом)
        
        Формат JSON:
        {
          "summary": "описание",
          "actions": ["действие 1", "действие 2", "действие 3"],
          "confidence": 0.85,
          "screen_type": "search_results|listing|form|article|dialog"
        }
        """
    
    def _format_response_for_speech(self, response: dict) -> str:
        """Форматирует ответ для озвучивания"""
        summary = response['summary']
        actions = response['actions']
        
        speech_text = f"{summary}. "
        speech_text += "Предлагаю: "
        
        for i, action in enumerate(actions, 1):
            speech_text += f"{i}) {action}. "
            
        speech_text += "Скажите: 'Повтори' или 'Кратко'"
        return speech_text
    
    def _play_audio(self, audio_bytes: bytes):
        """Воспроизводит аудио"""
        # Реализация воспроизведения
        pass
    
    def _handle_error(self, error: Exception):
        """Обрабатывает ошибки"""
        error_message = f"Произошла ошибка: {str(error)}"
        audio = self.tts_engine.synthesize(error_message)
        self._play_audio(audio)
```

## Конфигурация системы

### Файл конфигурации:

```json
{
  "modules": {
    "ui_capture": "pyobjc_capture",
    "image_processor": "pillow_processor", 
    "llm_client": "gemini_client",
    "speech_recognition": "speechrecognition_local",
    "tts_engine": "system_tts",
    "state_manager": "sqlite_manager",
    "space_controller": "event_tap_controller"
  },
  "llm": {
    "provider": "gemini",
    "model": "gemini-1.5-flash",
    "api_key": "env:GEMINI_API_KEY",
    "max_tokens": 500,
    "temperature": 0.3
  },
  "speech": {
    "recognition_language": "ru",
    "timeout": 10.0,
    "phrase_time_limit": 15.0,
    "energy_threshold": 4000,
    "dynamic_energy_threshold": true,
    "pause_threshold": 0.8
  },
  "tts": {
    "default_language": "ru",
    "default_speed": 1.0,
    "default_voice": "system_default"
  },
  "space_control": {
    "long_press_threshold": 0.6,
    "short_press_threshold": 0.3,
    "text_field_protection": true,
    "accessibility_api": true
  },
  
  "ui": {
    "notifications": true,
    "status_indicator": true
  },
  "cache": {
    "max_size_mb": 100,
    "cleanup_interval_hours": 24,
    "compression": true
  }
}
```

## Требования к производительности

### Общие требования:

**Время отклика:**
- Захват экрана: ≤300 мс
- Обработка изображения: ≤200 мс
- Анализ LLM: ≤1500 мс
- Синтез речи: ≤300 мс
- Распознавание речи: ≤1000 мс
- **Итого до первой фразы: ≤2.3 секунды**
- **Итого до ответа на команду: ≤1.3 секунды**

**Память:**
- Максимальное использование: ≤500MB
- Кэш изображений: ≤100MB
- Настройки: ≤10MB

**CPU:**
- Фоновый режим: ≤2%
- Активный режим: ≤15%
- Пиковые нагрузки: ≤30%

**Сеть:**
- LLM запросы: ≤2MB/запрос
- Обновления: ≤10MB/месяц
- Статистика: ≤1KB/день

## Безопасность и приватность

### Принципы:

**1. Локальность:**
- Все данные хранятся локально
- Скриншоты не уходят в облако без согласия
- Настройки пользователя защищены

**2. Шифрование:**
- API ключи шифруются
- Настройки пользователя шифруются
- Кэш изображений не шифруется (локально)

**3. Разрешения:**
- Запрос минимальных прав
- Объяснение необходимости каждого права
- Возможность отключения функций

**4. Логирование:**
- Логи не содержат чувствительных данных
- Возможность отключения логирования
- Автоочистка старых логов

## Тестирование

### Стратегия тестирования:

**1. Модульное тестирование:**
- Каждый модуль тестируется отдельно
- Mock объекты для зависимостей
- Покрытие кода ≥80%

**2. Интеграционное тестирование:**
- Тестирование взаимодействия модулей
- End-to-end сценарии
- Тестирование производительности

**3. Пользовательское тестирование:**
- Тестирование с незрячими пользователями
- Оценка удобства использования
- Сбор обратной связи

**4. Автоматизированное тестирование:**
- CI/CD pipeline
- Автоматические тесты при каждом коммите
- Регрессионное тестирование

## Развёртывание

### Способы распространения:

**1. DMG файл:**
- Подписанный .app
- Автоматическая установка
- Проверка совместимости

**2. App Store:**
- Официальное распространение
- Автоматические обновления
- Проверка Apple

**3. Автообновления:**
- Проверка новых версий
- Фоновая загрузка
- Автоматическая установка

### Требования к системе:

**macOS:**
- Версия: 12.0+ (Monterey)
- Архитектура: Intel/Apple Silicon
- Память: ≥8GB RAM
- Диск: ≥2GB свободного места

**Разрешения:**
- Доступность (Accessibility) - для определения контекста
- Запись экрана (Screen Recording) - для анализа экрана
- Микрофон - для распознавания речи

## Заключение

**Модульная архитектура "мозаика" позволяет:**
- Легко заменять компоненты
- Тестировать по частям
- Масштабировать функциональность
- Поддерживать разные конфигурации

**Ключевые преимущества:**
- Гибкость в выборе инструментов
- Чёткие интерфейсы
- Простота поддержки
- Возможность развития

**Новая архитектура MVP 1:**
- **Локальный STT** - SpeechRecognition + Google Speech API
- **Управление пробелом** - Event Tap + Accessibility API
- **Клиент-сервер** - gRPC только для LLM анализа
- **Быстрый отклик** - всё работает локально

**Следующие шаги:**
1. Реализация интерфейсов модулей
2. Создание базовых реализаций
3. Интеграция в оркестратор
4. Тестирование и оптимизация

---

# Планы разработки MVP

## План MVP 1: Умное управление пробелом (3 недели)

### **Цель:** Простой, надёжный ассистент с умным управлением через пробел

### **Концепция управления:**
```
🎯 Долгое нажатие пробела → Активирует ассистента (читает экран)
🎯 Короткий пробел → Перебивает ассистента (слушает вопрос)
🎯 Долгое нажатие (когда активен) → Выключает ассистента полностью
```

### **Принцип работы:**
- **Фоновый режим** → ассистент работает всегда в фоне
- **Одна кнопка** → пробел для всех действий
- **Простая логика** → долгий = активация/выключение, короткий = перебивание
- **Быстрое перебивание** → короткий пробел и ассистент замолкает
- **Умная защита** → в текстовых полях пробел работает как обычно

### **Логика состояний:**
```
🔄 inactive → Долгое нажатие → active (активируется)
🔄 active → Короткий пробел → interrupted (перебит)
🔄 active → Долгое нажатие → inactive (выключение)
🔄 interrupted → Короткий пробел → listening (слушает)
🔄 listening → Короткий пробел → processing (обрабатывает)
```

---

### **Неделя 1: Базовая система**

#### **День 1-2: Фоновая система управления пробелом**
```python
# Фоновый режим работы
- Ассистент работает всегда в фоне
- Долгое нажатие пробела → активация ассистента
- Короткий пробел → перебивание ассистента
- Долгое нажатие (когда активен) → выключение
- Защита текстовых полей → пробел работает как обычно
- Определение контекста через Accessibility API
```

**Результат:** Фоновая система управления через пробел без конфликтов

#### **День 3-4: Захват экрана**
```python
# Система захвата экрана
- PyObjC + Core Graphics
- Только активное окно
- Сжатие до 1024px, WebP формат
- Метаданные: приложение, заголовок окна
```

**Результат:** Можно захватывать экран и получать информацию об окне

#### **День 5-7: Базовая запись голоса**
```python
# Простая запись аудио
- PyAudio для записи
- 16kHz, моно, float32
- Запись в буфер при долгом нажатии пробела (активация)
- Перебивание коротким пробелом
- Автоматическая остановка и обработка
```

**Результат:** Базовая запись голоса работает с правильным управлением

---

### **Неделя 2: LLM и анализ**

#### **День 1-3: Интеграция с Gemini**
```python
# LLM клиент
- Google Generative AI
- Простой промпт для анализа экрана
- JSON ответ: описание + действия
- Обработка ошибок API
```

**Результат:** LLM анализирует скриншоты и даёт осмысленные ответы

#### **День 4-5: TTS система**
```python
# Синтез речи
- Системный NSSpeechSynthesizer
- Поддержка RU/EN
- Настраиваемая скорость
- Остановка речи по кнопке
```

**Результат:** Ассистент может озвучивать ответы

#### **День 6-7: Интеграция компонентов**
```python
# Связываем всё вместе
- Долгое нажатие пробела → активация ассистента
- Короткий пробел → перебивание ассистента
- Долгое нажатие (когда активен) → выключение
- Автоматическая обработка → анализ экрана → LLM → TTS
- Обработка ошибок на каждом этапе
```

**Результат:** Полный цикл работы ассистента с правильным управлением пробелом

---

### **Неделя 3: Качество и тестирование**

#### **День 1-3: Улучшение качества**
```python
# Оптимизация
- Качество изображения (WebP 80%)
- Скорость TTS (настраиваемая)
- Обработка ошибок сети
- Логирование всех действий
```

**Результат:** Стабильная работа, хорошее качество

#### **День 4-7: Тестирование и отладка**
```python
# Тестирование
- Тест на разных приложениях
- Тест с разными размерами окон
- Тест стабильности (100+ циклов)
- Подготовка к демонстрации
```

**Результат:** MVP 1 готов к показу пользователям

---

### **Что получаем в MVP 1:**

✅ **Рабочий ассистент** в фоновом режиме
✅ **Управление одним пробелом** - долгий = активация/выключение, короткий = перебивание
✅ **Анализ экрана** через LLM
✅ **Голосовые ответы** на основе экрана
✅ **Быстрое перебивание** речи одним нажатием
✅ **Умная защита** от конфликтов с текстом
✅ **Готов к тестированию** с незрячими пользователями

### **Примеры использования:**

#### **Сценарий 1: Активация и чтение экрана**
```
1. Пользователь долго держит пробел
   → Ассистент активируется
   → Читает что на экране
   → Состояние: "active"
```

#### **Сценарий 2: Перебивание речи**
```
2. Пользователь нажимает короткий пробел во время речи
   → Ассистент замолкает
   → Начинает слушать вопрос
   → Состояние: "interrupted"
```

#### **Сценарий 3: Полное выключение**
```
3. Пользователь долго держит пробел (когда ассистент активен)
   → Ассистент полностью выключается
   → Состояние: "inactive"
```

---

---

## План MVP 2: Голосовое взаимодействие (4 недели)

### **Цель:** Полноценный голосовой ассистент с естественным диалогом

---

### **Неделя 1: Эхоподавление и шумоподавление**

#### **День 1-3: WebRTC AEC (эхоподавление)**
```python
# Акустическое эхоподавление
- WebRTC AEC компонент
- Двухпотоковая архитектура (микрофон + динамики)
- Настройка подавления эха 95%+
- Дрейф-компенсация для стабильности
```

**Результат:** Ассистент не слышит свой голос

#### **День 4-5: WebRTC NS (шумоподавление)**
```python
# Шумоподавление
- WebRTC NS компонент
- Многоуровневое подавление шума 90%+
- Высокочастотный фильтр (80 Hz)
- Спектральные ворота для чистоты
```

**Результат:** Чистый голос без фоновых шумов

#### **День 6-7: Интеграция AEC + NS**
```python
# Связываем компоненты
- Микрофон → AEC → NS → VAD
- Динамики → AEC (для эхоподавления)
- Адаптивные настройки качества
- Тестирование в разных условиях
```

**Результат:** Стабильная система очистки аудио

---

### **Неделя 2: Голосовое прерывание**

#### **День 1-3: VAD (детекция речи)**
```python
# Детекция речи пользователя
- WebRTC VAD с настраиваемой агрессивностью
- Адаптивный порог детекции
- Защита от ложных срабатываний
- Временная валидация речи
```

**Результат:** Точное определение когда пользователь говорит

#### **День 4-5: Система прерывания**
```python
# Логика прерывания
- Мониторинг речи во время TTS
- Мгновенная остановка при детекции
- Подтверждение прерывания
- Возврат к прослушиванию
```

**Результат:** Ассистент прерывается голосом пользователя

#### **День 6-7: STT (распознавание речи)**
```python
# Преобразование речи в текст
- Whisper API или локальный Whisper
- Поддержка RU/EN
- Обработка команд и вопросов
- Контекстное понимание
```

**Результат:** Ассистент понимает что говорит пользователь

---

### **Неделя 3: Диалоговая система**

#### **День 1-3: Контекстный LLM**
```python
# Умный анализ и ответы
- Сохранение контекста диалога
- Понимание уточняющих вопросов
- Адаптивные ответы
- Обработка сложных запросов
```

**Результат:** Ассистент ведёт осмысленный диалог

#### **День 4-5: Система команд**
```python
# Голосовые команды
- "Что на экране?" → анализ экрана
- "Опиши подробнее" → детальное описание
- "Что изменилось?" → сравнение с предыдущим
- "Стоп" → прерывание
```

**Результат:** Пользователь управляет ассистентом голосом

#### **День 6-7: Интеграция диалога**
```python
# Связываем все компоненты
- Голосовой ввод → STT → LLM → TTS
- Прерывание в любой момент
- Контекстный диалог
- Обработка ошибок
```

**Результат:** Полноценный голосовой ассистент

---

### **Неделя 4: Качество и тестирование**

#### **День 1-3: Оптимизация качества**
```python
# Финальная настройка
- Качество эхоподавления
- Скорость реакции на прерывание
- Точность распознавания речи
- Естественность диалога
```

**Результат:** Высокое качество взаимодействия

#### **День 4-7: Тестирование и отладка**
```python
# Комплексное тестирование
- Тест в шумных условиях
- Тест с разными голосами
- Тест стабильности прерываний
- Тест с незрячими пользователями
```

**Результат:** MVP 2 готов к продакшену

---

### **Что получаем в MVP 2:**

✅ **Полноценный голосовой ассистент** с естественным диалогом
✅ **Идеальное эхоподавление** (95%+)
✅ **Качественное шумоподавление** (90%+)
✅ **Голосовое прерывание** в любой момент
✅ **Контекстное понимание** и умные ответы
✅ **Готов к продакшену** и массовому использованию

---

## Сравнение планов

### **MVP 1 (Умное управление пробелом):**
- **Время:** 3 недели
- **Сложность:** Низкая
- **Надёжность:** Очень высокая
- **Цель:** Быстрый MVP для тестирования

### **MVP 2 (Голосовое взаимодействие):**
- **Время:** 4 недели
- **Сложность:** Высокая
- **Надёжность:** Очень высокая
- **Цель:** Продакшен-качество

## Рекомендация по разработке

### **Последовательность разработки:**

1. **Начать с MVP 1** - получить работающий продукт за 3 недели
2. **Протестировать** с незрячими пользователями
3. **Собрать обратную связь** и понять потребности
4. **Разработать MVP 2** на основе полученного опыта

### **Преимущества такого подхода:**

✅ **Быстрый результат** - MVP 1 за 3 недели
✅ **Раннее тестирование** - понимание потребностей
✅ **Постепенное развитие** - от простого к сложному
✅ **Минимизация рисков** - если что-то не работает, исправляем на раннем этапе

### **Технические особенности:**

**MVP 1:**
- Фоновый режим работы (Event Tap)
- Управление пробелом (долгий = активация/выключение, короткий = перебивание)
- Защита текстовых полей через Accessibility API
- Перебивание речи одним нажатием
- Базовая запись голоса
- Системный TTS
- Gemini API для анализа

**MVP 2:**
- WebRTC AEC + NS
- VAD для детекции речи
- STT для понимания команд
- Контекстный LLM
- Голосовое прерывание

**Это идеальный план для поэтапной разработки качественного продукта!**
